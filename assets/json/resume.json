{
  "basics": {
    "name": "Srinivasan Subramaniyan",
    "label": "Ph.D. Candidate in Computer Engineering",
    "image": "",
    "email": "subramaniyan.4@osu.edu",
    "phone": "+1 740 274 2814",
    "url": "https://srinivasans74.github.io/",
    "summary": "Ph.D. candidate in Electrical and Computer Engineering at The Ohio State University, focusing on GPU scheduling, data center efficiency, and LLM/AI systems. Awarded Best Paper honors at EMSOFT 2025 and VLSID 2022. Seeking internships in computer engineering, software systems, or high-performance computing.",
    "location": {
      "address": "2015 Neil Ave",
      "postalCode": "43210",
      "city": "Columbus",
      "countryCode": "US",
      "region": "Ohio"
    },
    "profiles": [
      {
        "network": "LinkedIn",
        "username": "srinivasan-subramaniyan22",
        "url": "https://www.linkedin.com/in/srinivasan-subramaniyan22"
      },
      {
        "network": "Google Scholar",
        "username": "Srinivasan Subramaniyan",
        "url": "https://scholar.google.com/citations?user=luGswuwAAAAJ"
      },
      {
        "network": "Personal Website",
        "username": "srinivasans74",
        "url": "https://srinivasans74.github.io/"
      }
    ]
  },

  "work": [
    {
      "name": "Advanced Micro Devices (AMD)",
      "position": "Research Intern",
      "url": "https://www.amd.com/",
      "startDate": "2022-05-01",
      "endDate": "2022-08-01",
      "summary": "Optimized the scheduling of GP-GPU kernels to accelerate graph-based applications, improving performance and efficiency.",
      "highlights": [
        "Worked on GPU workload scheduling optimizations for graph-based applications.",
        "Enhanced compute efficiency and resource utilization for AMD research workloads."
      ]
    },
    {
      "name": "Centre for Heterogeneous and Intelligent Processing Systems (CHIPS)",
      "position": "Junior Research Fellow",
      "url": "https://chips.pes.edu/",
      "startDate": "2019-01-01",
      "endDate": "2021-08-01",
      "summary": "Conducted research on FPGA-based acceleration for communication and ML workloads.",
      "highlights": [
        "Performed design-space exploration for NB-LDPC codes on FPGAs (SIPS ’20, IEEE Design & Test ’22).",
        "Developed hardware accelerators for sparse matrix multiplication (VLSID ’22)."
      ]
    }
  ],

  "education": [
    {
      "institution": "The Ohio State University",
      "location": "Columbus, Ohio, USA",
      "url": "https://ece.osu.edu/",
      "area": "Computer Engineering",
      "studyType": "M.S. + Ph.D.",
      "startDate": "2021-08-01",
      "endDate": "",
      "courses": [
        "Computer Architecture",
        "Embedded Systems",
        "Parallel and Distributed Systems",
        "Operating Systems",
        "High-Performance Computing",
        "Reinforcement Learning and Machine Learning",
        "FPGA/SoC Design and Performance Modeling"
      ]
    }
  ],

  "skills": [
    {
      "name": "Programming Languages",
      "level": "",
      "keywords": ["C/C++", "Python", "Bash", "x86/ARM/RISC-V Assembly"]
    },
    {
      "name": "Hardware Design & Verification",
      "level": "",
      "keywords": ["Verilog", "SystemVerilog", "FPGA/SoC Design", "Hardware Simulation & Debugging"]
    },
    {
      "name": "Parallel & Distributed Computing",
      "level": "",
      "keywords": ["OpenCL", "CUDA", "OpenMP", "HIP", "MPI"]
    },
    {
      "name": "Optimization & Modeling",
      "level": "",
      "keywords": ["Gurobi", "PuLP", "Simulink", "Performance Profilers (gprof, perf, NVProf, Nsight)"]
    },
    {
      "name": "Development & Tools",
      "level": "",
      "keywords": ["Git", "Linux Kernel Modules", "Docker", "Kubernetes", "vLLM"]
    }
  ],

  "awards": [
    {
      "title": "Outstanding Paper Award, EMSOFT 2025",
      "date": "2025-10-01",
      "awarder": "ACM/IEEE Embedded Systems Week",
      "url": "https://dl.acm.org/doi/pdf/10.1145/3761812",
      "summary": "Recognized for the paper 'FC-GPU: Feedback Control GPU Scheduling for Real-time Embedded Systems'."
    },
    {
      "title": "Best Paper Award, VLSID 2022",
      "date": "2022-01-10",
      "awarder": "International Conference on VLSI Design and Embedded Systems",
      "summary": "Received the A.K. Choudhary Best Paper Award for FPGA accelerator design for sparse matrix multiplication."
    },
    {
      "title": "EMSOFT Travel Grant Award",
      "date": "2025-09-15",
      "awarder": "Embedded Systems Week",
      "summary": "Awarded travel support for presenting research at EMSOFT 2025."
    },
    {
      "title": "BurnLin Travel Grant Award",
      "date": "2025-05-01",
      "awarder": "The Ohio State University",
      "summary": "Awarded three consecutive years (2023–2025) for research excellence in embedded and parallel systems."
    },
    {
      "title": "Amrita Scholarship",
      "date": "2018-05-01",
      "awarder": "Amrita Vishwa Vidyapeetham",
      "summary": "Merit-based undergraduate scholarship recognizing academic excellence."
    }
  ],

  "publications": [
    {
      "name": "FC-GPU: Feedback Control GPU Scheduling for Real-time Embedded Systems",
      "publisher": "EMSOFT 2025",
      "releaseDate": "2025-10-01",
      "url": "https://dl.acm.org/doi/pdf/10.1145/3761812",
      "summary": "Proposed FC-GPU, a feedback-control framework for real-time GPU scheduling that reduces deadline misses by 2% using MIMO-based control."
    },
    {
      "name": "Exploiting ML Task Correlation in the Minimization of Capital Expense for GPU Data Centers",
      "publisher": "IEEE IPCCC 2025",
      "releaseDate": "2025-08-01",
      "summary": "Introduced CorrGPU, a correlation-aware scheduling algorithm that reduces CapEx by 20.88% in large-scale ML workloads."
    },
    {
      "name": "Power Capping of GPU Servers for Machine Learning Inference Optimization",
      "publisher": "ICPP 2025",
      "releaseDate": "2025-09-01",
      "summary": "Proposed CapGPU, a coordinated CPU–GPU power-capping framework that improves inference throughput by up to 20% under latency constraints."
    },
    {
      "name": "Latency-Guaranteed Co-Location of Inference and Training for Reducing Data Center Expenses",
      "publisher": "IEEE ICDCS 2024",
      "releaseDate": "2024-07-01",
      "url": "https://par.nsf.gov/servlets/purl/10560010",
      "summary": "Developed GPUColo, a co-location framework that enables training and inference sharing on GPUs, saving up to 74.9% of GPU resources."
    }
  ],

  "projects": [
    {
      "name": "FC-GPU: Feedback-Control GPU Scheduling",
      "summary": "Developed FC-GPU, the first feedback-control GPU scheduling framework for real-time systems using a MIMO controller to dynamically adapt task rates, reducing deadline misses by 2%.",
      "highlights": ["Best Paper Candidate, EMSOFT 2025", "Reduced deadline misses by 2% on RTX 3090 and MI100."],
      "url": "https://dl.acm.org/doi/pdf/10.1145/3761812"
    },
    {
      "name": "CapLLM: Power-Capping for LLM Data Centers",
      "summary": "Designed CapLLM, a power-capping framework for LLM-serving data centers that minimizes performance violations while improving energy efficiency.",
      "highlights": ["Dynamic GPU power management", "Improved SLA compliance"],
      "url": ""
    },
    {
      "name": "CorrGPU: Correlation-Aware GPU Scheduling",
      "summary": "Proposed CorrGPU, a scheduling algorithm that consolidates correlated workloads to reduce contention and lower CapEx by 20.88%.",
      "highlights": ["Correlation-aware scheduling", "Reduced CapEx by 20.88%"],
      "url": ""
    },
    {
      "name": "CapGPU: Coordinated CPU–GPU Power Capping",
      "summary": "Implemented CapGPU, a coordinated CPU–GPU power-capping strategy improving inference throughput by 8–20% while maintaining latency SLOs.",
      "highlights": ["8–20% throughput improvement", "Maintained SLO compliance"],
      "url": ""
    },
    {
      "name": "GPUColo: GPU Co-Location Framework",
      "summary": "Built GPUColo, a co-location framework that enables training and inference workloads to share GPUs, saving up to 74.9% of GPUs with strict SLO compliance.",
      "highlights": ["74.9% GPU savings", "Reduced CapEx with SLO guarantees"],
      "url": "https://par.nsf.gov/servlets/purl/10560010"
    }
    // {
    //   "name": "SEEB-GPU: Edge Inference Optimization",
    //   "summary": "Developed SEEB-GPU, an edge inference framework that optimizes batching, early exits, and GPU partitioning to reduce latency by up to 15× while ensuring SLA compliance.",
    //   "highlights": ["Reduced latency by up to 15×", "Ensured SLA compliance"],
    //   "url": ""
    // }
  ],

  "languages": [
    { "language": "English", "fluency": "Fluent" },
    { "language": "Tamil", "fluency": "Native" },
    { "language": "Hindi", "fluency": "Professional working proficiency" }
  ]
}
